{"cells":[{"cell_type":"markdown","metadata":{"id":"Q5IskweL_5o9"},"source":["# ***Install Library***\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7OKmB0P06iB"},"outputs":[],"source":["!pip install -qq transformers\n","!pip install -qq vncorenlp\n","!pip install -qq torchsummaryX"]},{"cell_type":"markdown","metadata":{"id":"YgmguNBBAMG0"},"source":["# ***Import Library***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LpmY5Wax9nL"},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","import seaborn as sns\n","import pickle\n","import numpy as np\n","\n","from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n","from vncorenlp import VnCoreNLP\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchsummaryX import summary\n","from torch import nn\n","from torch.nn import Linear, Dropout, ReLU, Softmax\n","import torch.nn.functional as F\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","from tqdm import tqdm_notebook\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from collections import defaultdict\n","import warnings\n","import logging\n","\n","logging.basicConfig(level=logging.ERROR)\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=UserWarning)\n","\n","%matplotlib inline\n","sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n","rcParams['figure.figsize'] = 14 ,8\n","\n","RANDOM_SEED = 50\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","tqdm_notebook().pandas()\n","\n","# Setup Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('Device: ', device)"]},{"cell_type":"markdown","metadata":{"id":"Bv5kjfj7FY1B"},"source":["# ***Creat Datasets***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsHZFafdF0iy"},"outputs":[],"source":["class Datasets(Dataset):\n","  def __init__(self, comments, labels, tokenizer, max_len):\n","    self.comments = comments\n","    self.labels = labels\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __getitem__(self, index):\n","    comment = str(self.comments[index])\n","    label = self.labels[index]\n","\n","    encoding = self.tokenizer.encode_plus(\n","        comment,\n","        max_length = self.max_len,\n","        add_special_tokens=True,\n","        pad_to_max_length=True,\n","        return_attention_mask=True,\n","        return_token_type_ids=False,\n","        truncation = True,\n","        return_tensors='pt'\n","    )\n","    return {\n","        'comment': comment,\n","        'input_id': encoding['input_ids'].flatten(),\n","        'attention_mask': encoding['attention_mask'].flatten(),\n","        'label': torch.tensor(label, dtype=torch.long)\n","    }\n","\n","  def __len__(self):\n","    return len(self.comments)\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","  datasets = Datasets(\n","      comments=df.comments.to_numpy(),\n","      labels=df.labels.to_numpy(),\n","      tokenizer=tokenizer,\n","      max_len=max_len\n","  )\n","  return DataLoader(\n","      datasets,\n","      batch_size=batch_size,\n","      num_workers=4\n","  )\n","\n","\n","def preprocess_data(df):\n","  \"\"\"\n","    Apply word segmenter to produce word-segmented texts before feeding to PhoBERT.\n","    Using RDRSegmenter from VNCoreNLP to pre-process the pre_training data.\n","  \"\"\"\n","  with VnCoreNLP(\"/content/drive/MyDrive/DATN/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') as rdrsegmenter:\n","    df[\"comments\"] = df[\"comments\"].apply(str).progress_apply(lambda x: ' '.join([' '.join(sent) for sent in rdrsegmenter.tokenize(x)]))\n","\n","  return df"]},{"cell_type":"markdown","metadata":{"id":"2KODJ5MlfhF4"},"source":["# ***Build Model***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Dr7wBZefgmO"},"outputs":[],"source":["class ClassifierModel(nn.Module):\n","  \"\"\"\n","  Arguments:\n","      bert (model): model BERT to extract features\n","      n_class (int): number of class\n","  \"\"\"\n","  def __init__(self, bert, n_class, drop_prob=0.3):\n","    super().__init__()\n","\n","    self.bert = bert\n","    self.drop_prob = drop_prob\n","    self.n_class = n_class\n","\n","    # Fully Connected Layers\n","    self.fc1 = Linear(in_features=768, out_features=512)\n","    self.fc2 = Linear(in_features=512, out_features=self.n_class)\n","    \n","    # Activate Function Layers\n","    self.relu = ReLU()\n","    self.soft_max = Softmax(dim=1)\n","\n","    # Dropout Layer\n","    self.drop_out = Dropout(self.drop_prob)\n","\n","  def forward(self, sentences_id, attention_mask):\n","    _, pooled_out = self.bert(sentences_id, attention_mask, return_dict=False)\n","    # pooled_out size: [batch_size, 768]\n","    x = self.drop_out(pooled_out)\n","    x = self.fc1(x)\n","    x = self.relu(x)\n","    x = self.drop_out(x)\n","    x = self.fc2(x)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"vux15i7MAbV6"},"source":["# ***Load and Preprocess Data***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Co4CERbhyNrE"},"outputs":[],"source":["data = pd.read_csv(\"/content/drive/MyDrive/DATN/data/data.csv\", header=0)\n","data = preprocess_data(data)\n","\n","df_train, df_val = train_test_split(data, test_size=0.1, random_state=RANDOM_SEED)\n","\n","print(data)"]},{"cell_type":"markdown","metadata":{"id":"qKNSSPaeBG7_"},"source":["# ***Plot Data***\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZyB3obb3lrp"},"outputs":[],"source":["ax = sns.countplot(data.labels)\n","class_name = ['Khong tot', 'Trung binh', 'Tot', 'Spam']\n","plt.ylabel(\"Amount\")\n","ax.set_xticklabels(class_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohYjXBxD5n6K"},"outputs":[],"source":["rcParams['figure.figsize'] = 32 ,12\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","line_len = []\n","for cmt in data.comments:\n","  line_len.append(len(cmt.split()))\n","\n","ax1 = sns.countplot(line_len)\n","plt.xlabel(\"Sentences Length\")\n","plt.ylabel(\"Amount\")"]},{"cell_type":"markdown","metadata":{"id":"PLOZw82UoV1t"},"source":["# ***Load Bert model***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeVGGv4roUm3"},"outputs":[],"source":["def load_bert(model_name):\n","  \"\"\"\n","    Pre-trained PhoBERT models are the state-of-the-art language models for Vietnamese\n","    (Pho, i.e. \"Phá»Ÿ\", is a popular food in Vietnam)\n","    Pre-trained name:\n","        PhoBERT Base:  \"vinai/phobert-base\"\n","        PhoBERT Large: \"vinai/phobert-large\"\n","  \"\"\"\n","  phobert = AutoModel.from_pretrained(model_name)\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  return tokenizer, phobert"]},{"cell_type":"markdown","metadata":{"id":"J1PPS2RX1-qy"},"source":["# ***Training***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72aw--sq209m"},"outputs":[],"source":["PRE_TRAINED_MODEL_NAME = \"vinai/phobert-base\"\n","BATCH_SIZE = 64\n","NUM_EPOCHS = 10\n","NUM_CLASS = 4\n","MAX_LEN = 50\n","\n","\n","# Load Pre_trained PhoBERT\n","tokenizer, phobert = load_bert(PRE_TRAINED_MODEL_NAME)\n","\n","#Create Data Loader\n","dl_train = create_data_loader(df_train, tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\n","dl_val = create_data_loader(df_val, tokenizer, max_len=MAX_LEN, batch_size=8)\n","n_samples = [len(df_train), len(df_val)]\n","\n","# Init Model\n","model = ClassifierModel(phobert, NUM_CLASS)\n","model = model.to(device)\n","\n","# Init Parameters\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","loss_fn = nn.CrossEntropyLoss().to(device)\n","\n","# Init Scheduler\n","total_steps = len(dl_train) * NUM_EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Db77yRf-tU0s"},"outputs":[],"source":["class Trainer():\n","  def __init__(self, model, \n","               train_loader, val_loader, \n","               epochs, loss_fn, optimizer,\n","               scheduler, n_samples,\n","               device, save_path):\n","    self.model = model.to(device)\n","    self.train_loader = train_loader\n","    self.val_loader = val_loader\n","    self.epochs = epochs\n","    self.loss_fn = loss_fn\n","    self.optimizer = optimizer\n","    self.n_samples = n_samples\n","    self.scheduler = scheduler\n","    self.device = device\n","    self.history = defaultdict(list)\n","    self.save_path = save_path\n","\n","  def train(self):\n","    self.model = self.model.train()\n","    losses = []\n","    correct_preds = 0\n","\n","    for step, data in enumerate(self.train_loader):\n","      print(f'\\rTraning Step {step+1}/{len(self.train_loader)}', end='')\n","\n","      # Data to device\n","      input_ids = data['input_id'].to(self.device)\n","      attention_masks = data['attention_mask'].to(self.device)\n","      targets = data['label'].to(self.device)\n","\n","      #Outputs model\n","      outputs = self.model(input_ids, attention_masks)\n","\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      loss = self.loss_fn(outputs, targets)\n","      correct_preds += torch.sum(preds == targets).detach().cpu().numpy()\n","      losses.append(loss.item())\n","\n","      loss.backward()\n","      nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","    \n","      self.optimizer.step()\n","      self.scheduler.step()\n","      self.optimizer.zero_grad()\n","\n","    return correct_preds/self.n_samples[0], np.mean(losses)\n","\n","  def evaluate(self):\n","    self.model = self.model.eval()\n","    losses = []\n","    correct_preds = 0\n","    with torch.no_grad():\n","      for data in self.val_loader:\n","\n","        input_ids = data['input_id'].to(self.device)\n","        attention_masks = data['attention_mask'].to(self.device)\n","        targets = data['label'].to(self.device)\n","\n","        outputs = self.model(input_ids, attention_masks)\n","\n","        _, preds = torch.max(outputs, dim=1)\n","\n","        loss = self.loss_fn(outputs, targets)\n","\n","        correct_preds += torch.sum(preds == targets).detach().cpu().numpy()\n","        losses.append(loss.item())\n","\n","    return correct_preds / self.n_samples[1], np.mean(losses)\n","\n","  def training(self):\n","    best_accuracy = 0.0\n","\n","    for epoch in range(self.epochs):\n","      print(f'\\nEpoch {epoch+1}/{self.epochs}')\n","      print(\"--\"*10)\n","\n","      # Train\n","      train_acc, train_loss = self.train()\n","      print(f'\\rTrain loss: {train_loss:.4f}, Train accuracy: {train_acc*100:.2f}')\n","\n","      #Valuate\n","      val_acc, val_loss = self.evaluate()\n","      print(f'Val   loss: {val_loss:.4f}, Val   accuracy: {val_acc*100:.2f}')\n","\n","      self.history['train_acc'].append(train_acc)\n","      self.history['train_loss'].append(train_loss)\n","      self.history['val_acc'].append(val_acc)\n","      self.history['val_loss'].append(val_loss)\n","\n","      model_path = os.path.join(self.save_path + str(val_acc) + \"best_model_state.bin\")\n","\n","      if val_acc > best_accuracy:\n","        torch.save(model.state_dict(), model_path)\n","        best_accuracy = val_acc\n","\n","    print('\\nCompleted Training')\n","    \n","    return self.history\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTtA03u42Mgj"},"outputs":[],"source":["save_path = '/content/drive/MyDrive/DATN/model'\n","train = Trainer(model, dl_train, dl_val, NUM_EPOCHS, loss_fn, optimizer, scheduler, n_samples, device, save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_OP1vPP2rMb"},"outputs":[],"source":["result = train.training()"]},{"cell_type":"code","source":["torch.save(model, '/content/drive/MyDrive/DATN/model/model.pt')"],"metadata":{"id":"LMX-iY2z-UNb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfFWKA_kvEUG"},"source":["# ***Plot Loss, Accuracy***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_f6fZUXvLea"},"outputs":[],"source":["rcParams['figure.figsize'] = 16 ,8\n","sns.set(style='whitegrid', palette='muted', font_scale=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X42NUI3vpmqT"},"outputs":[],"source":["plt.plot(result['train_acc'], label='train accuracy')\n","plt.plot(result['val_acc'], label='validation accuracy')\n","\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0zL5iy_uyfm"},"outputs":[],"source":["plt.plot(result['train_loss'], label='train loss')\n","plt.plot(result['val_loss'], label='validation loss')\n","\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1]);"]},{"cell_type":"markdown","metadata":{"id":"ZlaXxBfUwNUg"},"source":["# ***Evaluation***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BspQ2n-wwMiU"},"outputs":[],"source":["def get_predictions(model, data_loader, device):\n","    model = model.eval()\n","\n","    cmt_texts = []\n","    predictions = []\n","    prediction_probs = []\n","    real_labels = []\n","\n","    with torch.no_grad():\n","        for sample in data_loader:\n","            comment = sample['comment']\n","            input_id = sample['input_id'].to(device)\n","            attention_mask = sample['attention_mask'].to(device)\n","            label = sample['label'].to(device)\n","\n","            outputs = model(input_id, attention_mask)\n","\n","            _, preds = torch.max(outputs, dim=1)\n","            probs = F.softmax(outputs, dim=1)\n","\n","            cmt_texts.extend(comment)\n","            predictions.extend(preds)\n","            prediction_probs.extend(probs)\n","            real_labels.extend(label)   \n","\n","    predictions = torch.stack(predictions).cpu()\n","    precition_probs = torch.stack(prediction_probs).cpu()\n","    real_labels = torch.stack(real_labels).cpu()\n","\n","    return cmt_texts, predictions, prediction_probs, real_labels\n","\n","def show_confusion_matrix(confusion_matrix):\n","  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n","  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n","  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n","  plt.ylabel('True label')\n","  plt.xlabel('Predicted label');"]},{"cell_type":"code","source":["class_names = ['Bad', 'Medium', 'Good', 'Spam']\n","\n","y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model,\n","                                                               dl_val,\n","                                                               device)\n","\n","print(classification_report(y_test, y_pred, target_names=class_names))\n","\n","cm = confusion_matrix(y_test, y_pred)\n","df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n","show_confusion_matrix(df_cm)                                       "],"metadata":{"id":"QHsSSE-735ch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textwrap import wrap\n","idx = 7\n","\n","review_text = y_review_texts[idx]\n","true_sentiment = y_test[idx]\n","pred_df = pd.DataFrame({\n","  'class_names': class_names,\n","  'values': y_pred_probs[idx].cpu().data.numpy().argmax()\n","})\n","\n","print(\"\\n\".join(wrap(review_text)))\n","print()\n","print(f'True label: {class_names[true_sentiment]}')"],"metadata":{"id":"NxufLMxg5fW4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Jcg2yK8p7BcF"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["qKNSSPaeBG7_"],"provenance":[],"private_outputs":true,"mount_file_id":"1HCa0tcno989FW2bbvf98CxT6-rTPjW52","authorship_tag":"ABX9TyNF36ZKzPN+iZQpmkyYsEEz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}